{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The goal of this file is to load the raw database files that ia in the `raw_data` folder, extract and format the data in a way that is usefull for training the model. The output files are saved in the `datasets` folder.\n",
    "\n",
    "Produces:\n",
    "- 'datasets/train.csv' : training data. 80% of the data.\n",
    "- 'datasets/test.csv' : testing data. 20% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyodbc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alex\\GitHub\\RNN-IRI\\Rutting\\data_preprocessing.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alex/GitHub/RNN-IRI/Rutting/data_preprocessing.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alex/GitHub/RNN-IRI/Rutting/data_preprocessing.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alex/GitHub/RNN-IRI/Rutting/data_preprocessing.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyodbc\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alex/GitHub/RNN-IRI/Rutting/data_preprocessing.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alex/GitHub/RNN-IRI/Rutting/data_preprocessing.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyodbc'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "# install pyodbc\n",
    "os.system('pip install pyodbc')\n",
    "import pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyodbc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alex\\GitHub\\RNN-IRI\\Rutting\\data_preprocessing.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alex/GitHub/RNN-IRI/Rutting/data_preprocessing.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m conn_str \u001b[39m=\u001b[39m (\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alex/GitHub/RNN-IRI/Rutting/data_preprocessing.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDRIVER=\u001b[39m\u001b[39m{\u001b[39m\u001b[39mMicrosoft Access Driver (*.mdb, *.accdb)};\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alex/GitHub/RNN-IRI/Rutting/data_preprocessing.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDBQ=./Bucket_114495.mdb;\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alex/GitHub/RNN-IRI/Rutting/data_preprocessing.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m )\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alex/GitHub/RNN-IRI/Rutting/data_preprocessing.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m conn \u001b[39m=\u001b[39m pyodbc\u001b[39m.\u001b[39mconnect(conn_str)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alex/GitHub/RNN-IRI/Rutting/data_preprocessing.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Get a list of all tables\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alex/GitHub/RNN-IRI/Rutting/data_preprocessing.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m cursor \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mcursor()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pyodbc' is not defined"
     ]
    }
   ],
   "source": [
    "conn_str = (\n",
    "    r'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};'\n",
    "    r'DBQ=./Bucket_114495.mdb;'\n",
    ")\n",
    "conn = pyodbc.connect(conn_str)\n",
    "\n",
    "# Get a list of all tables\n",
    "cursor = conn.cursor()\n",
    "table_list = [row.table_name for row in cursor.tables(tableType='TABLE')]\n",
    "\n",
    "print(table_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and write the IRI data csv file to overwrite the MON_HSS_PROFILE_SECTION parquet file\n",
    "iri_data = pd.read_csv('raw_data/IRI_DATA.csv')\n",
    "iri_data.to_parquet('./datasets/MON_HSS_PROFILE_SECTION.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rut_data = pd.read_csv('raw_data/Materials.csv')\n",
    "rut_data.to_parquet('./datasets/MATERIALS.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all data from parquet files\n",
    "data_dir = './datasets/'\n",
    "data = {}\n",
    "for file in tqdm(os.listdir(data_dir)):\n",
    "    if file.endswith('.parquet'):\n",
    "        data[\".\".join(file.split('.')[:-1])] = pd.read_parquet(data_dir + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "This will happen in a few steps.\n",
    "\n",
    "- Weather data will be loaded and processed into a single useful dataframe.\n",
    "- IRI dataframe will be loaded and columns from other dataframes will be added to it.\n",
    "- The final result will be saved to another parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMIDITY = data['CLM_VWS_HUMIDITY_ANNUAL'].set_index(['SHRP_ID', 'STATE_CODE'])[['MAX_ANN_HUM_AVG', 'MIN_ANN_HUM_AVG']]\n",
    "HUMIDITY = HUMIDITY.groupby(['SHRP_ID', 'STATE_CODE']).mean()\n",
    "print(HUMIDITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE = data['CLM_VWS_TEMP_ANNUAL'].set_index(['SHRP_ID', 'STATE_CODE'])[['MEAN_ANN_TEMP_AVG', 'FREEZE_THAW_YR']]\n",
    "TEMPERATURE = TEMPERATURE.groupby(['SHRP_ID', 'STATE_CODE']).mean()\n",
    "print(TEMPERATURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRECIPIATION = data['CLM_VWS_PRECIP_ANNUAL'].set_index(['SHRP_ID', 'STATE_CODE'])[['TOTAL_ANN_PRECIP', 'TOTAL_SNOWFALL_YR']]\n",
    "PRECIPIATION = PRECIPIATION.groupby(['SHRP_ID', 'STATE_CODE']).mean()\n",
    "print(PRECIPIATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSTRUCTION_MATERIAL = data['MATERIALS'].set_index(['SHRP_ID', 'STATE_CODE'])[['CONSTRUCTION_NO', 'LAYER_NO', 'LAYER_TYPE']]\n",
    "# filter for the largest construction number\n",
    "CONSTRUCTION_MATERIAL = CONSTRUCTION_MATERIAL[\n",
    "    CONSTRUCTION_MATERIAL['CONSTRUCTION_NO'] == CONSTRUCTION_MATERIAL.groupby(['SHRP_ID', 'STATE_CODE'])['CONSTRUCTION_NO'].transform('max')]\n",
    "# filter for the largest layer number\n",
    "CONSTRUCTION_MATERIAL = CONSTRUCTION_MATERIAL[\n",
    "    CONSTRUCTION_MATERIAL['LAYER_NO'] == CONSTRUCTION_MATERIAL.groupby(['SHRP_ID', 'STATE_CODE'])['LAYER_NO'].transform('max')]\n",
    "CONSTRUCTION_MATERIAL = CONSTRUCTION_MATERIAL[['LAYER_TYPE']]\n",
    "# replace with index to convert Character codes to numeric codes\n",
    "CONSTRUCTION_MATERIAL['LAYER_TYPE'] = pd.factorize(CONSTRUCTION_MATERIAL['LAYER_TYPE'])[0]\n",
    "print(CONSTRUCTION_MATERIAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUT = data['MON_T_PROF_INDEX_SECTION'].set_index(['SHRP_ID', 'STATE_CODE'])[['MAX_MEAN_DEPTH_WIRE_REF']]\n",
    "RUT = RUT.groupby(['SHRP_ID', 'STATE_CODE']).mean()\n",
    "print(RUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_DATA_DIR = './training_data/'\n",
    "\n",
    "\n",
    "IRI = data['MON_HSS_PROFILE_SECTION'].set_index(['SHRP_ID', 'STATE_CODE'])\n",
    "IRI = IRI[['VISIT_DATE', 'IRI_LEFT_WHEEL_PATH', 'IRI_RIGHT_WHEEL_PATH']]\n",
    "IRI.reset_index(inplace=True)\n",
    "IRI['VISIT_DATE'] = pd.to_datetime(IRI['VISIT_DATE'], format='%m/%d/%Y')\n",
    "IRI['IRI_LEFT_WHEEL_PATH'] = IRI['IRI_LEFT_WHEEL_PATH'].astype(float)\n",
    "IRI['IRI_RIGHT_WHEEL_PATH'] = IRI['IRI_RIGHT_WHEEL_PATH'].astype(float)\n",
    "IRI = IRI.groupby(['SHRP_ID', 'STATE_CODE', 'VISIT_DATE'], as_index=False).agg({\n",
    "    'IRI_LEFT_WHEEL_PATH': 'mean',\n",
    "    'IRI_RIGHT_WHEEL_PATH': 'mean'\n",
    "})\n",
    "IRI.set_index(['SHRP_ID', 'STATE_CODE'], inplace=True)\n",
    "\n",
    "IRI.to_parquet(FINAL_DATA_DIR + 'IRI-only.parquet')\n",
    "IRI.to_csv(FINAL_DATA_DIR + 'IRI-only.csv')\n",
    "\n",
    "ESAL = data['TRF_HIST_EST_ESAL'].set_index(['SHRP_ID', 'STATE_CODE'])\n",
    "ESAL = ESAL['AADT_ALL_VEHIC']\n",
    "\n",
    "CRACK = data['MON_DIS_AC_CRACK_INDEX'].set_index(['SHRP_ID', 'STATE_CODE'])\n",
    "CRACK = CRACK['MEPDG_TRANS_CRACK_LENGTH_AC']\n",
    "CRACK = CRACK.astype(float)\n",
    "\n",
    "CONSTRUCTION = data['CONSTRUCTION_HIST'].set_index(['SHRP_ID', 'STATE_CODE'])\n",
    "CONSTRUCTION = CONSTRUCTION[['IMP_DATE', 'IMP_TYPE']]\n",
    "\n",
    "# Merge traffic and IRI\n",
    "final = IRI.merge(ESAL, how='left', left_index=True, right_index=True)\n",
    "# Merge crack data\n",
    "final = final.merge(CRACK, how='left', left_index=True, right_index=True)\n",
    "# Merge weather data\n",
    "final = final.merge(HUMIDITY, how='left', left_index=True, right_index=True)\n",
    "final = final.merge(TEMPERATURE, how='left', left_index=True, right_index=True)\n",
    "final = final.merge(PRECIPIATION, how='left', left_index=True, right_index=True)\n",
    "# Merge construction material data\n",
    "final = final.merge(CONSTRUCTION_MATERIAL, how='left', left_index=True, right_index=True)\n",
    "# Merge rutting data\n",
    "final = final.merge(RUT, how='left', left_index=True, right_index=True)\n",
    "\n",
    "final = final.reset_index()\n",
    "\n",
    "\n",
    "final = final.groupby(['SHRP_ID',\n",
    "                       'STATE_CODE',\n",
    "                       'VISIT_DATE', \n",
    "                       'IRI_LEFT_WHEEL_PATH',\n",
    "                       'IRI_RIGHT_WHEEL_PATH',\n",
    "                       'MAX_ANN_HUM_AVG',\n",
    "                       'MIN_ANN_HUM_AVG',\n",
    "                       'MEAN_ANN_TEMP_AVG',\n",
    "                       'FREEZE_THAW_YR',\n",
    "                       'TOTAL_ANN_PRECIP',\n",
    "                       'TOTAL_SNOWFALL_YR',\n",
    "                       'LAYER_TYPE',\n",
    "                       'MAX_MEAN_DEPTH_WIRE_REF'], as_index=False).agg({\n",
    "    'AADT_ALL_VEHIC': 'mean',\n",
    "    'MEPDG_TRANS_CRACK_LENGTH_AC': 'mean'\n",
    "})\n",
    "\n",
    "final.set_index(['SHRP_ID', 'STATE_CODE'], inplace=True)\n",
    "# remove duplicates and de-NAN the values\n",
    "final = final.fillna(-1)\n",
    "final = final[~final.duplicated(keep='first')]\n",
    "# replace MEPDG_TRANS_CRACK_LENGTH_AC 0s with -1s\n",
    "final['MEPDG_TRANS_CRACK_LENGTH_AC'].replace(0, -1, inplace=True)\n",
    "\n",
    "# save to parquet and csv\n",
    "final.to_parquet(FINAL_DATA_DIR + 'final_data.parquet')\n",
    "final.to_csv(FINAL_DATA_DIR + 'final_data.csv')\n",
    "\n",
    "# # save construction data to parquet and csv\n",
    "CONSTRUCTION.to_parquet(FINAL_DATA_DIR + 'construction_data.parquet')\n",
    "CONSTRUCTION.to_csv(FINAL_DATA_DIR + 'construction_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
