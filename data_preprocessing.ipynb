{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The goal of this file is to load the raw database files that ia in the `raw_data` folder, extract and format the data in a way that is usefull for training the model. The output files are saved in the `datasets` folder.\n",
    "\n",
    "Produces:\n",
    "- 'datasets/train.csv' : training data. 80% of the data.\n",
    "- 'datasets/test.csv' : testing data. 20% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, inspect\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from SQL server\n",
    "server = 'localhost'\n",
    "database_names = ['Bucket_110914_1', 'Bucket_110914_2'] #Bucket_<ID>_<#>\n",
    "username = 'SA'\n",
    "password = input('Enter password: ')\n",
    "port = '1433'\n",
    "driver = 'ODBC+Driver+17+for+SQL+Server'\n",
    "\n",
    "data = {}\n",
    "for database in tqdm(database_names):\n",
    "    engine = create_engine(f'mssql+pyodbc://{username}:{password}@{server}:{port}/{database}?driver={driver}')\n",
    "    \n",
    "    inspector = inspect(engine)\n",
    "    table_names = inspector.get_table_names()\n",
    "\n",
    "    # Create a dictionary of dataframes\n",
    "    dfs = {}\n",
    "\n",
    "    # Loop through table names and for each table, execute a SQL query and load the result into a pandas DataFrame\n",
    "    for table in tqdm(table_names):\n",
    "        query = f'SELECT * FROM {table}'\n",
    "        dfs[table] = pd.read_sql_query(query, engine)\n",
    "    data = {**data, **dfs}\n",
    "\n",
    "#load data from spreadsheet\n",
    "data = {**data, **pd.read_excel('raw_data/Bucket_110915.xlsx', sheet_name=None), **pd.read_excel('raw_data/Bucket_11100.xlsx', sheet_name=None), 'CONSTRUCTION_HIST': pd.read_csv('raw_data/Bucket_11101.csv')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write all data to parquet files\n",
    "data = {'CONSTRUCTION_HIST': pd.read_csv('raw_data/Bucket_111001.csv')}\n",
    "for table in tqdm(data):\n",
    "    data[table].to_parquet(f'./datasets/{table}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and write the IRI data csv file to overwrite the MON_HSS_PROFILE_SECTION parquet file\n",
    "iri_data = pd.read_csv('raw_data/IRI_DATA.csv')\n",
    "iri_data.to_parquet('./datasets/MON_HSS_PROFILE_SECTION.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all data from parquet files\n",
    "data_dir = './datasets/'\n",
    "data = {}\n",
    "for file in tqdm(os.listdir(data_dir)):\n",
    "    if file.endswith('.parquet'):\n",
    "        data[\".\".join(file.split('.')[:-1])] = pd.read_parquet(data_dir + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "This will happen in a few steps.\n",
    "\n",
    "- Weather data will be loaded and processed into a single useful dataframe.\n",
    "- IRI dataframe will be loaded and columns from other dataframes will be added to it.\n",
    "- The final result will be saved to another parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to use\n",
    "# {'MON_HSS_PROFILE_SECTION.parquet':\n",
    "#   Index(['VISIT_DATE', 'STATE_CODE', 'SHRP_ID', 'MRI'], dtype='object'),\n",
    "# 'TRF_HIST_EST_ESAL.parquet':\n",
    "#  Index(['STATE_CODE', 'SHRP_ID', 'YEAR_HIST_EST', 'AADT_ALL_VEHIC'], dtype='object'),\n",
    "# 'MON_DIS_AC_CRACK_INDEX.parquet':\n",
    "#  Index(['STATE_CODE', 'SHRP_ID', 'SURVEY_DATE', 'MEPDG_TRANS_CRACK_LENGTH_AC'], dtype='object'),\n",
    "# 'CONSTRUCTION_HIST.parquet': Index(['STATE_CODE', 'SHRP_ID', 'IMP_DATE', 'IMP_TYPE'], dtype='object')}\n",
    "\n",
    "# The goal is to merge them all together with SHRP_ID and STATE_CODE as the primary keys\n",
    "\n",
    "FINAL_DATA_DIR = './training_data/'\n",
    "IRI = data['MON_HSS_PROFILE_SECTION'].set_index(['SHRP_ID', 'STATE_CODE'])\n",
    "IRI = IRI[['VISIT_DATE', 'IRI_LEFT_WHEEL_PATH', 'IRI_RIGHT_WHEEL_PATH']]\n",
    "IRI.reset_index(inplace=True)\n",
    "IRI['VISIT_DATE'] = pd.to_datetime(IRI['VISIT_DATE'], format='%m/%d/%Y')\n",
    "IRI['IRI_LEFT_WHEEL_PATH'] = IRI['IRI_LEFT_WHEEL_PATH'].astype(float)\n",
    "IRI['IRI_RIGHT_WHEEL_PATH'] = IRI['IRI_RIGHT_WHEEL_PATH'].astype(float)\n",
    "IRI = IRI.groupby(['SHRP_ID', 'STATE_CODE', 'VISIT_DATE'], as_index=False).agg({\n",
    "    'IRI_LEFT_WHEEL_PATH': 'mean',\n",
    "    'IRI_RIGHT_WHEEL_PATH': 'mean'\n",
    "})\n",
    "IRI.set_index(['SHRP_ID', 'STATE_CODE'], inplace=True)\n",
    "\n",
    "IRI.to_parquet(FINAL_DATA_DIR + 'IRI-only.parquet')\n",
    "IRI.to_csv(FINAL_DATA_DIR + 'IRI-only.csv')\n",
    "\n",
    "ESAL = data['TRF_HIST_EST_ESAL'].set_index(['SHRP_ID', 'STATE_CODE'])\n",
    "ESAL = ESAL['AADT_ALL_VEHIC']\n",
    "\n",
    "CRACK = data['MON_DIS_AC_CRACK_INDEX'].set_index(['SHRP_ID', 'STATE_CODE'])\n",
    "CRACK = CRACK['MEPDG_TRANS_CRACK_LENGTH_AC']\n",
    "CRACK = CRACK.astype(float)\n",
    "\n",
    "CONSTRUCTION = data['CONSTRUCTION_HIST'].set_index(['SHRP_ID', 'STATE_CODE'])\n",
    "CONSTRUCTION = CONSTRUCTION[['IMP_DATE', 'IMP_TYPE']]\n",
    "\n",
    "# Merge traffic and IRI\n",
    "final = IRI.merge(ESAL, how='left', left_index=True, right_index=True)\n",
    "# Merge crack data\n",
    "final = final.merge(CRACK, how='left', left_index=True, right_index=True)\n",
    "\n",
    "final = final.reset_index()\n",
    "\n",
    "\n",
    "final = final.fillna(-1)\n",
    "final = final.groupby(['SHRP_ID', 'STATE_CODE', 'VISIT_DATE', 'IRI_LEFT_WHEEL_PATH', 'IRI_RIGHT_WHEEL_PATH'], as_index=False).agg({\n",
    "    'AADT_ALL_VEHIC': 'mean',\n",
    "    'MEPDG_TRANS_CRACK_LENGTH_AC': 'mean'\n",
    "})\n",
    "\n",
    "final.set_index(['SHRP_ID', 'STATE_CODE'], inplace=True)\n",
    "# remove duplicates and de-NAN the values\n",
    "final = final[~final.duplicated(keep='first')]\n",
    "# replace MEPDG_TRANS_CRACK_LENGTH_AC 0s with -1s\n",
    "final['MEPDG_TRANS_CRACK_LENGTH_AC'].replace(0, -1, inplace=True)\n",
    "\n",
    "# save to parquet and csv\n",
    "final.to_parquet(FINAL_DATA_DIR + 'final_data.parquet')\n",
    "final.to_csv(FINAL_DATA_DIR + 'final_data.csv')\n",
    "\n",
    "# save construction data to parquet and csv\n",
    "CONSTRUCTION.to_parquet(FINAL_DATA_DIR + 'construction_data.parquet')\n",
    "CONSTRUCTION.to_csv(FINAL_DATA_DIR + 'construction_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
