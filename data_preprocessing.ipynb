{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The goal of this file is to load the raw database files that ia in the `raw_data` folder, extract and format the data in a way that is usefull for training the model. The output files are saved in the `datasets` folder.\n",
    "\n",
    "Produces:\n",
    "- 'datasets/train.csv' : training data. 80% of the data.\n",
    "- 'datasets/test.csv' : testing data. 20% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, inspect\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from SQL server\n",
    "server = 'localhost'\n",
    "database_names = ['Bucket_110914_1', 'Bucket_110914_2'] #Bucket_<ID>_<#>\n",
    "username = 'SA'\n",
    "password = input('Enter password: ')\n",
    "port = '1433'\n",
    "driver = 'ODBC+Driver+17+for+SQL+Server'\n",
    "\n",
    "data = {}\n",
    "for database in tqdm(database_names):\n",
    "    engine = create_engine(f'mssql+pyodbc://{username}:{password}@{server}:{port}/{database}?driver={driver}')\n",
    "    \n",
    "    inspector = inspect(engine)\n",
    "    table_names = inspector.get_table_names()\n",
    "\n",
    "    # Create a dictionary of dataframes\n",
    "    dfs = {}\n",
    "\n",
    "    # Loop through table names and for each table, execute a SQL query and load the result into a pandas DataFrame\n",
    "    for table in tqdm(table_names):\n",
    "        query = f'SELECT * FROM {table}'\n",
    "        dfs[table] = pd.read_sql_query(query, engine)\n",
    "    data = {**data, **dfs}\n",
    "\n",
    "#load data from spreadsheet\n",
    "data = {**data, **pd.read_excel('raw_data/Bucket_110915.xlsx', sheet_name=None)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write all data to parquet files\n",
    "for table in tqdm(data):\n",
    "    data[table].to_parquet(f'./datasets/{table}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all data from parquet files\n",
    "data_dir = './datasets/'\n",
    "data = {}\n",
    "for file in tqdm(os.listdir(data_dir)):\n",
    "    if file.endswith('.parquet'):\n",
    "        data[\".\".join(file.split('.')[:-1])] = pd.read_parquet(data_dir + file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "This will happen in a few steps.\n",
    "\n",
    "- Weather data will be loaded and processed into a single useful dataframe.\n",
    "- IRI dataframe will be loaded and columns from other dataframes will be added to it.\n",
    "- The final result will be saved to another parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
