{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a basic RNN model used to test dataset preperation and model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import full_iri_dataset_generator as iri\n",
    "from training_loop import train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "- `SEQUENCE_LENGTH` is the number of historical measurements before the target element to provide to the model\n",
    "- `NUM_FREATURES_PER_SAMPLE` is how many details each measurement has. `IRI-only` has 3: left_iri, right_iri, and time_since_first_measurement\n",
    "- `NUM_LAYERS` is the number of RNN layers to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 10\n",
    "NUM_FEATURES_PER_SAMPLE = 6\n",
    "NUM_LAYERS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preperation\n",
    "\n",
    "Load train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = iri.load_iri_datasets(path=\"../training_data/final_data.parquet\",\n",
    "                                    construction_path=\"../training_data/construction_data.parquet\",\n",
    "                                    seq_length=SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "Here a basic RNN classifier model is defined.\n",
    "\n",
    "1. Data is flattened\n",
    "2. RNN layers process data and modify hidden state\n",
    "3. final layer maps hidden state to predicted left and right wheel IRIs\n",
    "4. outputs are scaled using a logsoftmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=SEQUENCE_LENGTH,\n",
    "                          hidden_size=SEQUENCE_LENGTH * NUM_FEATURES_PER_SAMPLE,\n",
    "                          num_layers=NUM_LAYERS,\n",
    "                          batch_first=True)\n",
    "        self.final = nn.Linear(SEQUENCE_LENGTH * NUM_FEATURES_PER_SAMPLE, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(NUM_LAYERS,\n",
    "                         x.size(0),\n",
    "                         SEQUENCE_LENGTH * NUM_FEATURES_PER_SAMPLE).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.final(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN()\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "train_model(model, train, test, loss, optimizer, epochs=200, test_every_n=10, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R^2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics import R2Score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def compute_r2_for(dataset):\n",
    "    r2 = R2Score()\n",
    "    train_data = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "    for _, data in enumerate(train_data):\n",
    "        inputs, goal = data[0], data[1]\n",
    "        outputs = model(inputs)\n",
    "        r2.update(goal, outputs)\n",
    "    return r2.compute()\n",
    "\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_r2 = compute_r2_for(train)\n",
    "    print(f\"R^2 for training data: {train_r2}\")\n",
    "    test_r2 = compute_r2_for(test)\n",
    "    print(f\"R^2 for testing data: {test_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics import MeanSquaredError\n",
    "\n",
    "def compute_mse_for(dataset):\n",
    "    mse = MeanSquaredError()\n",
    "    train_data = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "    for _, data in enumerate(train_data):\n",
    "        inputs, goal = data[0], data[1]\n",
    "        outputs = model(inputs)\n",
    "        mse.update(outputs, goal)\n",
    "    return mse.compute()\n",
    "\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_mse = compute_mse_for(train)\n",
    "    print(f\"MSE for training data: {train_mse}\")\n",
    "    test_mse = compute_mse_for(test)\n",
    "    print(f\"MSE for testing data: {test_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def compute_errors_for(dataset):\n",
    "    train_data = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "    errors = []\n",
    "    for _, data in enumerate(train_data):\n",
    "        inputs, goal = data[0], data[1]\n",
    "        outputs = np.array(model(inputs))\n",
    "        errors.append(np.abs((outputs - np.array(goal)).mean(axis=1)))\n",
    "    return np.concatenate(errors)\n",
    "\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_errors = compute_errors_for(train)\n",
    "    test_errors = compute_errors_for(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_errors, bins=100, label=\"train\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_errors, bins=100, label=\"test\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Details of train errors:\")\n",
    "train_errors_df = pd.DataFrame(train_errors)\n",
    "train_errors_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Details of test errors:\")\n",
    "test_errors_df = pd.DataFrame(test_errors)\n",
    "test_errors_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Evaluation\n",
    "\n",
    "This was made for testing to compare this to the direct classification model to debug performance issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = 0\n",
    "wrong = 0\n",
    "\n",
    "\n",
    "def increment_category_values(predicted, target):\n",
    "    global right, wrong\n",
    "\n",
    "    def get_slot(mean):\n",
    "        boundries = [1.5, 2.68]\n",
    "        if mean < (boundries[0] - iri.mean_iri) / iri.iri_range:\n",
    "            return 0\n",
    "        elif mean < (boundries[1] - iri.mean_iri) / iri.iri_range:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "        \n",
    "    if get_slot(predicted.mean()) == get_slot(target.mean()):\n",
    "        right += 1\n",
    "    else:\n",
    "        wrong += 1\n",
    "\n",
    "def evaluate_categorical_accuracy(dataset):\n",
    "    global right, wrong\n",
    "    right = 0\n",
    "    wrong = 0\n",
    "    train_data = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "    for _, data in enumerate(train_data):\n",
    "        inputs, goal = data[0], data[1]\n",
    "        outputs = model(inputs)\n",
    "        for pred, target in zip(outputs, goal):\n",
    "            increment_category_values(pred, target)\n",
    "    return right / (right + wrong)\n",
    "\n",
    "device = device = torch.device(\"cuda\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_classification_accuracy = evaluate_categorical_accuracy(train)\n",
    "    print(f'Classification Accuracy of the network on the train data: {100 * train_classification_accuracy}%')\n",
    "    train_classification_accuracy = evaluate_categorical_accuracy(test)\n",
    "    print(f'Classification Accuracy of the network on the test data: {100 * train_classification_accuracy}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
