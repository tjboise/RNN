{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a basic RNN model Designed to develop evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-64c04cdf836f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mSEQUENCE_LENGTH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_iri_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../training_data/IRI-only.parquet\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEQUENCE_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/RNN-IRI/model/iri_dataset_generator.py\u001b[0m in \u001b[0;36mload_iri_datasets\u001b[0;34m(seed, train_split, path, seq_length, one_hot)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mLoads\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mIRI\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtest\u001b[0m \u001b[0mpytorch\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \"\"\"\n\u001b[0;32m--> 458\u001b[0;31m     \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m     return impl.read(\n\u001b[1;32m    460\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_nullable_dtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_nullable_dtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mget_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0merror_msgs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n - \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         raise ImportError(\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0;34m\"Unable to find a usable engine; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;34m\"tried using: 'pyarrow', 'fastparquet'.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import iri_dataset_generator as iri\n",
    "from training_loop import train_model\n",
    "\n",
    "SEQUENCE_LENGTH = 10\n",
    "train, test = iri.load_iri_datasets(path=\"../training_data/IRI-only.parquet\",seq_length=SEQUENCE_LENGTH)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=SEQUENCE_LENGTH, hidden_size=30, num_layers=5, batch_first=True)\n",
    "        self.final = nn.Linear(30, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(5, x.size(0), 30).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.final(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FNN()\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "train_model(model, train, test, loss, optimizer, epochs=200, test_every_n=10, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics import R2Score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_r2 = R2Score()\n",
    "test_r2 = R2Score()\n",
    "\n",
    "device = device = torch.device(\"cuda\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    right = 0\n",
    "    total = 0\n",
    "    train_data = DataLoader(train, batch_size=256, shuffle=True)\n",
    "    for _, data in enumerate(train_data):\n",
    "        inputs, goal = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(inputs)\n",
    "        train_r2.update(outputs, goal)\n",
    "    for _, data in enumerate(test_data):\n",
    "        inputs, goal = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(inputs)\n",
    "        test_r2.update(outputs, goal)\n",
    "\n",
    "print(train_r2.compute())\n",
    "print(test_r2.compute())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = 0\n",
    "wrong = 0\n",
    "\n",
    "\n",
    "def compute_correctness(predicted, target):\n",
    "    global right, wrong\n",
    "\n",
    "    def get_slot(mean):\n",
    "        boundries = [1.5, 2.68]\n",
    "        if mean < (boundries[0] - iri.mean_iri) / iri.iri_range:\n",
    "            return 0\n",
    "        elif mean < (boundries[1] - iri.mean_iri) / iri.iri_range:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "        \n",
    "    if get_slot(predicted.mean()) == get_slot(target.mean()):\n",
    "        right += 1\n",
    "    else:\n",
    "        wrong += 1\n",
    "\n",
    "device = device = torch.device(\"cuda\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    right = 0\n",
    "    total = 0\n",
    "    train_data = DataLoader(train, batch_size=256, shuffle=True)\n",
    "    for _, data in enumerate(train_data):\n",
    "        inputs, goal = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(inputs)\n",
    "        for pred, target in zip(outputs, goal):\n",
    "            compute_correctness(pred, target)\n",
    "            total += 1\n",
    "    print(f'Accuracy of the network on the train data: {100 * right / total} %')\n",
    "    test_data = DataLoader(test, batch_size=256, shuffle=True)\n",
    "    right = 0\n",
    "    total = 0\n",
    "    for _, data in enumerate(test_data):\n",
    "        inputs, goal = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(inputs)\n",
    "        for pred, target in zip(outputs, goal):\n",
    "            compute_correctness(pred, target)\n",
    "            total += 1\n",
    "    print(f'Accuracy of the network on the test data: {100 * right / total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
